{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks with keras\n",
    "\n",
    "\n",
    "This will illustrate the basics of a convolutional neural networks using keras to create a digit classification network. WITHOUT A LOT OF MATH\n",
    "\n",
    "## What is a Convolutional whatnot?\n",
    "\n",
    "A better question is this: \n",
    "\n",
    "Is the image below a dog or a cat?\n",
    "\n",
    "![Dog Image](https://www.what-dog.net/Images/faces2/scroll0015.jpg)\n",
    "\n",
    "\n",
    "Obviously it's a dog!\n",
    "\n",
    "\n",
    "\n",
    "It took you, a human, no time at all to say that that is a dog. Our brains are cool like that. So how did you know that's a dog? Well, you were taught what dogs look like, vs what cats look like, by your parents. When you asked what that fluffy thing was, they told you it was a dog, and the same goes for the cat.\n",
    "\n",
    "Ok, easy, teach babies what cats and dogs are. Not that hard, right? Well, yes. Our brains, even at a young age are var more advanced than computers. So how would you teach a computer?\n",
    "\n",
    "\n",
    "## Computers vs. Humans: Vision\n",
    "\n",
    "If you know the physics behind our eyes, you will know that our eyes take in wavelengths of light that bounce off objects around us, and our brains interpret it. It's not hard for us to see because that process is done subconsciously. It's very easy to recognize images just by looking at them, but computers don't have the same ability.\n",
    "\n",
    "A picture like this:\n",
    "![Lion Image](https://a-z-animals.com/media/animals/images/original/lion7.jpg)\n",
    "\n",
    "Is easy to recognize as a lion.\n",
    "\n",
    "\n",
    "For computers it's a bit different. Images are stored as a bunch of numbers.\n",
    "\n",
    "![Matrix](http://chem4823.usask.ca/images/matrices1.gif)\n",
    "\n",
    "Try classifiying this, you can't. A, it means nothing, its just a bunch of random numbers I found on Google. B, it's not in a format you understand.\n",
    "\n",
    "Computers have to take images as a bunch of numbers. Which they can understand. So how can computers classify images? Well they use something called Neural Networks.\n",
    "\n",
    "## Computers vs. Humans: Networks\n",
    "\n",
    "The neural network actually stems from biology, and is the basis for how our brain learns.\n",
    "\n",
    "![Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Neuron.svg/1200px-Neuron.svg.png)\n",
    "\n",
    "The neural network a computer uses follows a similar pattern. Input comes from a previous neuron and gets transferred through, with a certain strength, or the stregth of the synapse between the two neurons.\n",
    "\n",
    "\n",
    "## Back to Convolutional whatnot...\n",
    "\n",
    "For classifiy images using a computer, we use a type of neural network called a Convolutional Neural Network (CNN). A CNN iterates over an image and sends pixel values to the network for classification.\n",
    "\n",
    "![Neural Network Image](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)\n",
    "\n",
    "![Network Image 2](http://cs231n.github.io/assets/cnn/convnet.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build a CNN with python and keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "from __future__ import print_function\n",
    "\n",
    "# We're imported our Keras submodules, such as Dense, Dropout, and MaxPooling2D. All things\n",
    "# we will need to make our network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, MaxPooling2D, Conv2D\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import rmsprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.utils as np_utils\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've imported our dependencies, we can get started. We're going so define some constants that we'll use later on in our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "number_of_classes = 10\n",
    "epochs = 200\n",
    "data_augmentation = True\n",
    "number_of_predictions = 20\n",
    "save_directory = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_first_cnn_test.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number_of_classes is the amount of classification classes we want. i.e. 0,1,2,3...10. Each one of those numbers is a class. We also define our save_directory and the model_name that we'll call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# We're going to shuffle our test data that keras provides into testing and training data randomly\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = np_utils.to_categorical(y_train, number_of_classes)\n",
    "y_test = np_utils.to_categorical(y_test, number_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Time\n",
    "\n",
    "We're going to create a Keras sequential model. With this we can add layers easily with the .add() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', input_shape=x_train.shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first layer is out initial convolutional layer. We're going to be following this basic format:\n",
    "\n",
    "![Hello](https://fr.mathworks.com/content/mathworks/fr/fr/discovery/convolutional-neural-network/jcr:content/mainParsys/image_copy.adapt.full.high.jpg/1506591540823.jpg)\n",
    "\n",
    "\n",
    "The only exception is that we will be implimenting dropout functions as well as some other additions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add an activation layer with ReLU\n",
    "model.add(Activation('relu'))\n",
    "# add Dnother convolutional layer\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "# add another activation layer now\n",
    "model.add(Activation('relu'))\n",
    "# perform maxPooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# finally add the dropout layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we just do?\n",
    "\n",
    "Well we added a ReLU activation layer, another convolutional layer with a filter of 32, and a kernal_size of [3,3], then another ReLU activation, and finally we added a MaxPooling layer with a pool size of [2,2]. Easy... ðŸ˜‰\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "It sounds like something you could say to your teacher to make them think you're some kind of genius, but it's actually quite simple. An activation function mimics the human brain. Inside the neuron, a chemical reaction takes place to determine if the signal should pass to the next neuron. That's what an activation function does in basic.\n",
    "\n",
    "We're using a ReLU (Rectified Linear Unit) activation function as it's what most closly mimics the brain.\n",
    "\n",
    "Some other activation functions include:\n",
    "\n",
    "* Sigmoid: https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "\n",
    "* Softplus\n",
    "\n",
    "* Gaussian\n",
    "\n",
    "* Softmax\n",
    "\n",
    "Some more info is listed here: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "\n",
    "## Pooling\n",
    "\n",
    "So what's pooling?\n",
    "\n",
    "Pooling is a way to reduce noise in an image in order to make analysis easier for the next layer.\n",
    "\n",
    "Our pooling layer, MaxPooling2D, using the Max Pooling algorithm to reduce extra information.\n",
    "![Max Pooling](https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png)\n",
    "\n",
    "\n",
    "It takes the largest value in the matrix and that is the new pixel value of the output matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the dropout layer now, and drop out 25%\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Function\n",
    "\n",
    "The dropout function is interesting because it prevents \"overfitting\" by doing something that may seem counter intuitive. The dropout function \"destroys\" a given percentage of the network setting weights back to 0. It's basically like smashing this delicatly constructed piece of art with a hammer. The reasoning for this is that it keeps the model from being to biased towards one type of class. By implementing a dropout, the model can better classify images of dogs, let's say, that it hasn't seen before. Overfitting can reduce the amount of correct classifications on data the network hasn't seen yet. Another analogy is taking the blinders off of a racehorse, now it can see so many more places to run, besides just straight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same as before just increase kernal size by 2\n",
    "model.add(Conv2D(64, (3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## our final layer bunch\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_classes))\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening a Layer\n",
    "\n",
    "Here, we flatten our layer and turn the vetor output of our last calculation into a scalar value, for use in the next layer.\n",
    "\n",
    "## Dense Layers\n",
    "\n",
    "A dense layer is just another name for a fully connected layer. A dense layer is where all input nodes are connected to all hidden nodes, which are connected to all output nodes. In our Dense layer, we specify our units, which are the dimensions of the output. i.e. (*, 512).\n",
    "\n",
    "The keras documentation shows\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(16,)))\n",
    "# now the model will take as input arrays of shape (*, 16)\n",
    "# and output arrays of shape (*, 32)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "## Softmax classification\n",
    "\n",
    "Our final activation is softmax. Softmax is multinomial logistic regression. Basically, we're trying to calculate the probabilities to which class it is. So for example it might output:\n",
    "\n",
    "```\n",
    "0: 0.98\n",
    "8: 0.70\n",
    "9: 0.56\n",
    "...\n",
    "```\n",
    "\n",
    "Meaning that the network thinks that with 98% accuracy, the input image is a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initiate RMSProp optimizer\n",
    "opt = rmsprop(lr=0.0001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS Optimization\n",
    "\n",
    "According to cs.toronto.edu:\n",
    "\n",
    "rmsprop: Divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.\n",
    "\n",
    "We want to minimize the output of our loss function so we're using RMS prop optimization, which is a type of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train the model now using RMSprop\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "Here, we're using a loss function called categorical cross entropy. We also use the optimizer as the optimizer we previously created `opt`. Our desired metrics are accuracy\n",
    "\n",
    "More on cross entropy here: https://en.wikipedia.org/wiki/Cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally we're going to convert our training data to a float32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/200\n",
      "1562/1562 [==============================] - 192s - loss: 1.8685 - acc: 0.3140 - val_loss: 1.5456 - val_acc: 0.4410\n",
      "Epoch 2/200\n",
      "1562/1562 [==============================] - 191s - loss: 1.5719 - acc: 0.4279 - val_loss: 1.3786 - val_acc: 0.5084\n",
      "Epoch 3/200\n",
      "1562/1562 [==============================] - 190s - loss: 1.4486 - acc: 0.4786 - val_loss: 1.2587 - val_acc: 0.5479\n",
      "Epoch 4/200\n",
      "1562/1562 [==============================] - 177s - loss: 1.3547 - acc: 0.5154 - val_loss: 1.1963 - val_acc: 0.5741\n",
      "Epoch 5/200\n",
      "1562/1562 [==============================] - 176s - loss: 1.2776 - acc: 0.5455 - val_loss: 1.1126 - val_acc: 0.6121\n",
      "Epoch 6/200\n",
      "1562/1562 [==============================] - 179s - loss: 1.2126 - acc: 0.5701 - val_loss: 1.0467 - val_acc: 0.6344\n",
      "Epoch 7/200\n",
      "1562/1562 [==============================] - 194s - loss: 1.1669 - acc: 0.5861 - val_loss: 1.0360 - val_acc: 0.6345\n",
      "Epoch 8/200\n",
      "1562/1562 [==============================] - 196s - loss: 1.1200 - acc: 0.6035 - val_loss: 0.9737 - val_acc: 0.6597\n",
      "Epoch 9/200\n",
      "1562/1562 [==============================] - 196s - loss: 1.0801 - acc: 0.6179 - val_loss: 0.9708 - val_acc: 0.6632\n",
      "Epoch 10/200\n",
      "1562/1562 [==============================] - 197s - loss: 1.0492 - acc: 0.6325 - val_loss: 0.9006 - val_acc: 0.6844\n",
      "Epoch 11/200\n",
      "1562/1562 [==============================] - 201s - loss: 1.0221 - acc: 0.6391 - val_loss: 0.8839 - val_acc: 0.6913\n",
      "Epoch 12/200\n",
      "1562/1562 [==============================] - 199s - loss: 0.9956 - acc: 0.6501 - val_loss: 0.8680 - val_acc: 0.6905\n",
      "Epoch 13/200\n",
      "1562/1562 [==============================] - 202s - loss: 0.9707 - acc: 0.6581 - val_loss: 0.8464 - val_acc: 0.7047\n",
      "Epoch 14/200\n",
      "1562/1562 [==============================] - 210s - loss: 0.9540 - acc: 0.6673 - val_loss: 0.8481 - val_acc: 0.7043\n",
      "Epoch 15/200\n",
      "1562/1562 [==============================] - 208s - loss: 0.9369 - acc: 0.6720 - val_loss: 0.8434 - val_acc: 0.7077\n",
      "Epoch 16/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.9197 - acc: 0.6758 - val_loss: 0.7926 - val_acc: 0.7213\n",
      "Epoch 17/200\n",
      "1562/1562 [==============================] - 201s - loss: 0.9121 - acc: 0.6809 - val_loss: 0.7938 - val_acc: 0.7190\n",
      "Epoch 18/200\n",
      "1562/1562 [==============================] - 203s - loss: 0.8914 - acc: 0.6873 - val_loss: 0.7836 - val_acc: 0.7283\n",
      "Epoch 19/200\n",
      "1562/1562 [==============================] - 201s - loss: 0.8831 - acc: 0.6939 - val_loss: 0.7481 - val_acc: 0.7398\n",
      "Epoch 20/200\n",
      "1562/1562 [==============================] - 196s - loss: 0.8756 - acc: 0.6946 - val_loss: 0.7531 - val_acc: 0.7359\n",
      "Epoch 21/200\n",
      "1562/1562 [==============================] - 196s - loss: 0.8628 - acc: 0.7022 - val_loss: 0.7294 - val_acc: 0.7529\n",
      "Epoch 22/200\n",
      "1562/1562 [==============================] - 197s - loss: 0.8600 - acc: 0.7036 - val_loss: 0.7294 - val_acc: 0.7545\n",
      "Epoch 23/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.8517 - acc: 0.7052 - val_loss: 0.7269 - val_acc: 0.7555\n",
      "Epoch 24/200\n",
      "1562/1562 [==============================] - 193s - loss: 0.8383 - acc: 0.7105 - val_loss: 0.7182 - val_acc: 0.7567\n",
      "Epoch 25/200\n",
      "1562/1562 [==============================] - 196s - loss: 0.8313 - acc: 0.7136 - val_loss: 0.7187 - val_acc: 0.7608\n",
      "Epoch 26/200\n",
      "1562/1562 [==============================] - 203s - loss: 0.8341 - acc: 0.7107 - val_loss: 0.7180 - val_acc: 0.7592\n",
      "Epoch 27/200\n",
      "1562/1562 [==============================] - 202s - loss: 0.8285 - acc: 0.7178 - val_loss: 0.6984 - val_acc: 0.7657\n",
      "Epoch 28/200\n",
      "1562/1562 [==============================] - 200s - loss: 0.8185 - acc: 0.7193 - val_loss: 0.6876 - val_acc: 0.7686\n",
      "Epoch 29/200\n",
      "1562/1562 [==============================] - 199s - loss: 0.8135 - acc: 0.7202 - val_loss: 0.6958 - val_acc: 0.7600\n",
      "Epoch 30/200\n",
      "1562/1562 [==============================] - 197s - loss: 0.8066 - acc: 0.7233 - val_loss: 0.6850 - val_acc: 0.7620\n",
      "Epoch 31/200\n",
      "1562/1562 [==============================] - 199s - loss: 0.8084 - acc: 0.7228 - val_loss: 0.6908 - val_acc: 0.7673\n",
      "Epoch 32/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.8052 - acc: 0.7263 - val_loss: 0.6664 - val_acc: 0.7746\n",
      "Epoch 33/200\n",
      "1562/1562 [==============================] - 190s - loss: 0.7983 - acc: 0.7277 - val_loss: 0.6823 - val_acc: 0.7643\n",
      "Epoch 34/200\n",
      "1562/1562 [==============================] - 193s - loss: 0.7983 - acc: 0.7272 - val_loss: 0.6877 - val_acc: 0.7720\n",
      "Epoch 35/200\n",
      "1562/1562 [==============================] - 204s - loss: 0.7901 - acc: 0.7301 - val_loss: 0.7059 - val_acc: 0.7680\n",
      "Epoch 36/200\n",
      "1562/1562 [==============================] - 206s - loss: 0.7947 - acc: 0.7274 - val_loss: 0.7050 - val_acc: 0.7624\n",
      "Epoch 37/200\n",
      "1562/1562 [==============================] - 209s - loss: 0.7864 - acc: 0.7323 - val_loss: 0.6498 - val_acc: 0.7800\n",
      "Epoch 38/200\n",
      "1562/1562 [==============================] - 214s - loss: 0.7902 - acc: 0.7303 - val_loss: 0.6730 - val_acc: 0.7721\n",
      "Epoch 39/200\n",
      "1562/1562 [==============================] - 210s - loss: 0.7842 - acc: 0.7320 - val_loss: 0.6510 - val_acc: 0.7802\n",
      "Epoch 40/200\n",
      "1562/1562 [==============================] - 206s - loss: 0.7781 - acc: 0.7348 - val_loss: 0.6502 - val_acc: 0.7836\n",
      "Epoch 41/200\n",
      "1562/1562 [==============================] - 210s - loss: 0.7758 - acc: 0.7370 - val_loss: 0.6764 - val_acc: 0.7735\n",
      "Epoch 42/200\n",
      "1562/1562 [==============================] - 213s - loss: 0.7732 - acc: 0.7389 - val_loss: 0.7012 - val_acc: 0.7684\n",
      "Epoch 43/200\n",
      "1562/1562 [==============================] - 209s - loss: 0.7731 - acc: 0.7388 - val_loss: 0.6582 - val_acc: 0.7774\n",
      "Epoch 44/200\n",
      "1562/1562 [==============================] - 208s - loss: 0.7640 - acc: 0.7400 - val_loss: 0.6868 - val_acc: 0.7659\n",
      "Epoch 45/200\n",
      "1562/1562 [==============================] - 208s - loss: 0.7702 - acc: 0.7395 - val_loss: 0.6524 - val_acc: 0.7787\n",
      "Epoch 46/200\n",
      "1562/1562 [==============================] - 208s - loss: 0.7636 - acc: 0.7405 - val_loss: 0.6544 - val_acc: 0.7865\n",
      "Epoch 47/200\n",
      "1562/1562 [==============================] - 206s - loss: 0.7670 - acc: 0.7397 - val_loss: 0.6694 - val_acc: 0.7754\n",
      "Epoch 48/200\n",
      "1562/1562 [==============================] - 205s - loss: 0.7598 - acc: 0.7419 - val_loss: 0.6275 - val_acc: 0.7929\n",
      "Epoch 49/200\n",
      "1562/1562 [==============================] - 204s - loss: 0.7670 - acc: 0.7423 - val_loss: 0.6406 - val_acc: 0.7856\n",
      "Epoch 50/200\n",
      "1562/1562 [==============================] - 206s - loss: 0.7611 - acc: 0.7436 - val_loss: 0.6271 - val_acc: 0.7913\n",
      "Epoch 51/200\n",
      "1562/1562 [==============================] - 204s - loss: 0.7628 - acc: 0.7408 - val_loss: 0.6712 - val_acc: 0.7823\n",
      "Epoch 52/200\n",
      "1562/1562 [==============================] - 208s - loss: 0.7554 - acc: 0.7445 - val_loss: 0.6915 - val_acc: 0.7904\n",
      "Epoch 53/200\n",
      "1562/1562 [==============================] - 213s - loss: 0.7611 - acc: 0.7434 - val_loss: 0.6636 - val_acc: 0.7758\n",
      "Epoch 54/200\n",
      "1562/1562 [==============================] - 208s - loss: 0.7577 - acc: 0.7445 - val_loss: 0.6462 - val_acc: 0.7863\n",
      "Epoch 55/200\n",
      "1562/1562 [==============================] - 211s - loss: 0.7610 - acc: 0.7429 - val_loss: 0.6342 - val_acc: 0.7880\n",
      "Epoch 56/200\n",
      "1562/1562 [==============================] - 207s - loss: 0.7562 - acc: 0.7457 - val_loss: 0.6407 - val_acc: 0.7911\n",
      "Epoch 57/200\n",
      "1562/1562 [==============================] - 210s - loss: 0.7567 - acc: 0.7449 - val_loss: 0.6716 - val_acc: 0.7911\n",
      "Epoch 58/200\n",
      "1562/1562 [==============================] - 208s - loss: 0.7471 - acc: 0.7486 - val_loss: 0.6496 - val_acc: 0.7819\n",
      "Epoch 59/200\n",
      "1562/1562 [==============================] - 212s - loss: 0.7555 - acc: 0.7467 - val_loss: 0.6798 - val_acc: 0.7722\n",
      "Epoch 60/200\n",
      "1562/1562 [==============================] - 214s - loss: 0.7552 - acc: 0.7477 - val_loss: 0.6603 - val_acc: 0.7787\n",
      "Epoch 61/200\n",
      "1562/1562 [==============================] - 205s - loss: 0.7566 - acc: 0.7465 - val_loss: 0.6242 - val_acc: 0.7915\n",
      "Epoch 62/200\n",
      "1562/1562 [==============================] - 208s - loss: 0.7536 - acc: 0.7487 - val_loss: 0.6689 - val_acc: 0.7944\n",
      "Epoch 63/200\n",
      "1562/1562 [==============================] - 208s - loss: 0.7496 - acc: 0.7511 - val_loss: 0.6365 - val_acc: 0.7897\n",
      "Epoch 64/200\n",
      "1562/1562 [==============================] - 210s - loss: 0.7584 - acc: 0.7482 - val_loss: 0.6879 - val_acc: 0.7660\n",
      "Epoch 65/200\n",
      "1562/1562 [==============================] - 212s - loss: 0.7586 - acc: 0.7471 - val_loss: 0.6527 - val_acc: 0.7857\n",
      "Epoch 66/200\n",
      "1562/1562 [==============================] - 212s - loss: 0.7580 - acc: 0.7455 - val_loss: 0.6874 - val_acc: 0.7752\n",
      "Epoch 67/200\n",
      "1562/1562 [==============================] - 211s - loss: 0.7572 - acc: 0.7462 - val_loss: 0.6414 - val_acc: 0.7898\n",
      "Epoch 68/200\n",
      "1562/1562 [==============================] - 207s - loss: 0.7528 - acc: 0.7465 - val_loss: 0.6302 - val_acc: 0.7892\n",
      "Epoch 69/200\n",
      "1562/1562 [==============================] - 207s - loss: 0.7521 - acc: 0.7494 - val_loss: 0.7072 - val_acc: 0.7822\n",
      "Epoch 70/200\n",
      "1562/1562 [==============================] - 212s - loss: 0.7473 - acc: 0.7514 - val_loss: 0.6887 - val_acc: 0.7844\n",
      "Epoch 71/200\n",
      "1562/1562 [==============================] - 208s - loss: 0.7508 - acc: 0.7496 - val_loss: 0.6378 - val_acc: 0.7900\n",
      "Epoch 72/200\n",
      "1562/1562 [==============================] - 209s - loss: 0.7586 - acc: 0.7469 - val_loss: 0.7218 - val_acc: 0.7575\n",
      "Epoch 73/200\n",
      "1562/1562 [==============================] - 199s - loss: 0.7538 - acc: 0.7492 - val_loss: 0.6720 - val_acc: 0.7838\n",
      "Epoch 74/200\n",
      "1562/1562 [==============================] - 205s - loss: 0.7607 - acc: 0.7455 - val_loss: 0.6814 - val_acc: 0.7808\n",
      "Epoch 75/200\n",
      "1562/1562 [==============================] - 207s - loss: 0.7612 - acc: 0.7462 - val_loss: 0.6778 - val_acc: 0.7818\n",
      "Epoch 76/200\n",
      "1562/1562 [==============================] - 202s - loss: 0.7608 - acc: 0.7474 - val_loss: 0.6270 - val_acc: 0.7946\n",
      "Epoch 77/200\n",
      "1562/1562 [==============================] - 199s - loss: 0.7546 - acc: 0.7471 - val_loss: 0.6681 - val_acc: 0.7925\n",
      "Epoch 78/200\n",
      "1562/1562 [==============================] - 205s - loss: 0.7597 - acc: 0.7489 - val_loss: 0.6998 - val_acc: 0.7806\n",
      "Epoch 79/200\n",
      "1562/1562 [==============================] - 207s - loss: 0.7644 - acc: 0.7468 - val_loss: 0.7081 - val_acc: 0.7662\n",
      "Epoch 80/200\n",
      "1562/1562 [==============================] - 208s - loss: 0.7631 - acc: 0.7476 - val_loss: 0.6609 - val_acc: 0.7815\n",
      "Epoch 81/200\n",
      "1562/1562 [==============================] - 210s - loss: 0.7596 - acc: 0.7454 - val_loss: 0.6710 - val_acc: 0.7893\n",
      "Epoch 82/200\n",
      "1562/1562 [==============================] - 210s - loss: 0.7650 - acc: 0.7477 - val_loss: 0.6879 - val_acc: 0.7837\n",
      "Epoch 83/200\n",
      "1562/1562 [==============================] - 209s - loss: 0.7643 - acc: 0.7452 - val_loss: 0.6589 - val_acc: 0.7885\n",
      "Epoch 84/200\n",
      "1562/1562 [==============================] - 211s - loss: 0.7650 - acc: 0.7465 - val_loss: 0.6623 - val_acc: 0.7830\n",
      "Epoch 85/200\n",
      "1562/1562 [==============================] - 204s - loss: 0.7659 - acc: 0.7472 - val_loss: 0.6395 - val_acc: 0.7856\n",
      "Epoch 86/200\n",
      "1562/1562 [==============================] - 204s - loss: 0.7660 - acc: 0.7457 - val_loss: 0.6613 - val_acc: 0.7842\n",
      "Epoch 87/200\n",
      "1562/1562 [==============================] - 199s - loss: 0.7715 - acc: 0.7462 - val_loss: 0.6884 - val_acc: 0.7727\n",
      "Epoch 88/200\n",
      "1562/1562 [==============================] - 200s - loss: 0.7700 - acc: 0.7446 - val_loss: 0.6589 - val_acc: 0.7845\n",
      "Epoch 89/200\n",
      "1562/1562 [==============================] - 202s - loss: 0.7660 - acc: 0.7468 - val_loss: 0.6621 - val_acc: 0.7788\n",
      "Epoch 90/200\n",
      "1562/1562 [==============================] - 205s - loss: 0.7718 - acc: 0.7444 - val_loss: 0.6820 - val_acc: 0.7785\n",
      "Epoch 91/200\n",
      "1562/1562 [==============================] - 203s - loss: 0.7680 - acc: 0.7454 - val_loss: 0.6724 - val_acc: 0.7761\n",
      "Epoch 92/200\n",
      "1562/1562 [==============================] - 191s - loss: 0.7771 - acc: 0.7453 - val_loss: 0.6839 - val_acc: 0.7731\n",
      "Epoch 93/200\n",
      "1562/1562 [==============================] - 202s - loss: 0.7755 - acc: 0.7470 - val_loss: 0.6687 - val_acc: 0.7739\n",
      "Epoch 94/200\n",
      "1562/1562 [==============================] - 195s - loss: 0.7802 - acc: 0.7447 - val_loss: 0.6431 - val_acc: 0.7944\n",
      "Epoch 95/200\n",
      "1562/1562 [==============================] - 194s - loss: 0.7808 - acc: 0.7436 - val_loss: 0.6621 - val_acc: 0.7937\n",
      "Epoch 96/200\n",
      "1562/1562 [==============================] - 194s - loss: 0.7814 - acc: 0.7430 - val_loss: 0.7354 - val_acc: 0.7715\n",
      "Epoch 97/200\n",
      "1562/1562 [==============================] - 190s - loss: 0.7841 - acc: 0.7415 - val_loss: 0.7163 - val_acc: 0.7700\n",
      "Epoch 98/200\n",
      "1562/1562 [==============================] - 193s - loss: 0.7848 - acc: 0.7441 - val_loss: 0.6978 - val_acc: 0.7804\n",
      "Epoch 99/200\n",
      "1562/1562 [==============================] - 195s - loss: 0.7830 - acc: 0.7422 - val_loss: 0.6736 - val_acc: 0.7797\n",
      "Epoch 100/200\n",
      "1562/1562 [==============================] - 196s - loss: 0.7828 - acc: 0.7441 - val_loss: 0.6450 - val_acc: 0.7895\n",
      "Epoch 101/200\n",
      "1562/1562 [==============================] - 199s - loss: 0.7894 - acc: 0.7429 - val_loss: 0.6867 - val_acc: 0.7727\n",
      "Epoch 102/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.7864 - acc: 0.7436 - val_loss: 0.6460 - val_acc: 0.7941\n",
      "Epoch 103/200\n",
      "1562/1562 [==============================] - 196s - loss: 0.7941 - acc: 0.7418 - val_loss: 0.6932 - val_acc: 0.7660\n",
      "Epoch 104/200\n",
      "1562/1562 [==============================] - 194s - loss: 0.7937 - acc: 0.7401 - val_loss: 0.7433 - val_acc: 0.7573\n",
      "Epoch 105/200\n",
      "1562/1562 [==============================] - 195s - loss: 0.7986 - acc: 0.7392 - val_loss: 0.7049 - val_acc: 0.7769\n",
      "Epoch 106/200\n",
      "1562/1562 [==============================] - 196s - loss: 0.7997 - acc: 0.7381 - val_loss: 0.7501 - val_acc: 0.7713\n",
      "Epoch 107/200\n",
      "1562/1562 [==============================] - 195s - loss: 0.7942 - acc: 0.7409 - val_loss: 0.7307 - val_acc: 0.7572\n",
      "Epoch 108/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.8003 - acc: 0.7392 - val_loss: 0.7337 - val_acc: 0.7746\n",
      "Epoch 109/200\n",
      "1562/1562 [==============================] - 200s - loss: 0.7999 - acc: 0.7389 - val_loss: 0.6775 - val_acc: 0.7845\n",
      "Epoch 110/200\n",
      "1562/1562 [==============================] - 205s - loss: 0.8038 - acc: 0.7377 - val_loss: 0.7315 - val_acc: 0.7702\n",
      "Epoch 111/200\n",
      "1562/1562 [==============================] - 205s - loss: 0.8107 - acc: 0.7350 - val_loss: 0.7121 - val_acc: 0.7727\n",
      "Epoch 112/200\n",
      "1562/1562 [==============================] - 201s - loss: 0.8077 - acc: 0.7367 - val_loss: 0.8521 - val_acc: 0.7440\n",
      "Epoch 113/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.8052 - acc: 0.7361 - val_loss: 0.7140 - val_acc: 0.7774\n",
      "Epoch 114/200\n",
      "1562/1562 [==============================] - 203s - loss: 0.8115 - acc: 0.7354 - val_loss: 0.6721 - val_acc: 0.7849\n",
      "Epoch 115/200\n",
      "1562/1562 [==============================] - 196s - loss: 0.8227 - acc: 0.7337 - val_loss: 0.7143 - val_acc: 0.7685\n",
      "Epoch 116/200\n",
      "1562/1562 [==============================] - 191s - loss: 0.8161 - acc: 0.7351 - val_loss: 0.7012 - val_acc: 0.7792\n",
      "Epoch 117/200\n",
      "1562/1562 [==============================] - 192s - loss: 0.8170 - acc: 0.7343 - val_loss: 0.7495 - val_acc: 0.7679\n",
      "Epoch 118/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.8256 - acc: 0.7336 - val_loss: 0.7454 - val_acc: 0.7662\n",
      "Epoch 119/200\n",
      "1562/1562 [==============================] - 197s - loss: 0.8271 - acc: 0.7319 - val_loss: 0.7534 - val_acc: 0.7635\n",
      "Epoch 120/200\n",
      "1562/1562 [==============================] - 190s - loss: 0.8278 - acc: 0.7300 - val_loss: 0.6842 - val_acc: 0.7727\n",
      "Epoch 121/200\n",
      "1562/1562 [==============================] - 191s - loss: 0.8277 - acc: 0.7299 - val_loss: 0.7285 - val_acc: 0.7743\n",
      "Epoch 122/200\n",
      "1562/1562 [==============================] - 189s - loss: 0.8316 - acc: 0.7296 - val_loss: 0.7509 - val_acc: 0.7760\n",
      "Epoch 123/200\n",
      "1562/1562 [==============================] - 192s - loss: 0.8343 - acc: 0.7286 - val_loss: 0.6713 - val_acc: 0.7871\n",
      "Epoch 124/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.8356 - acc: 0.7300 - val_loss: 0.7073 - val_acc: 0.7822\n",
      "Epoch 125/200\n",
      "1562/1562 [==============================] - 192s - loss: 0.8372 - acc: 0.7283 - val_loss: 0.7705 - val_acc: 0.7625\n",
      "Epoch 126/200\n",
      "1562/1562 [==============================] - 189s - loss: 0.8415 - acc: 0.7271 - val_loss: 0.7946 - val_acc: 0.7577\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 194s - loss: 0.8458 - acc: 0.7263 - val_loss: 0.7943 - val_acc: 0.7500\n",
      "Epoch 128/200\n",
      "1562/1562 [==============================] - 193s - loss: 0.8528 - acc: 0.7253 - val_loss: 0.7413 - val_acc: 0.7670\n",
      "Epoch 129/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.8448 - acc: 0.7263 - val_loss: 0.8722 - val_acc: 0.7321\n",
      "Epoch 130/200\n",
      "1562/1562 [==============================] - 197s - loss: 0.8581 - acc: 0.7197 - val_loss: 0.7748 - val_acc: 0.7540\n",
      "Epoch 131/200\n",
      "1562/1562 [==============================] - 200s - loss: 0.8578 - acc: 0.7232 - val_loss: 0.6820 - val_acc: 0.7845\n",
      "Epoch 132/200\n",
      "1562/1562 [==============================] - 202s - loss: 0.8689 - acc: 0.7215 - val_loss: 0.7012 - val_acc: 0.7727\n",
      "Epoch 133/200\n",
      "1562/1562 [==============================] - 203s - loss: 0.8726 - acc: 0.7196 - val_loss: 0.7177 - val_acc: 0.7649\n",
      "Epoch 134/200\n",
      "1562/1562 [==============================] - 201s - loss: 0.8729 - acc: 0.7198 - val_loss: 0.8310 - val_acc: 0.7472\n",
      "Epoch 135/200\n",
      "1562/1562 [==============================] - 200s - loss: 0.8777 - acc: 0.7184 - val_loss: 0.7902 - val_acc: 0.7387\n",
      "Epoch 136/200\n",
      "1562/1562 [==============================] - 201s - loss: 0.8817 - acc: 0.7150 - val_loss: 0.7175 - val_acc: 0.7723\n",
      "Epoch 137/200\n",
      "1562/1562 [==============================] - 197s - loss: 0.8830 - acc: 0.7157 - val_loss: 0.7161 - val_acc: 0.7723\n",
      "Epoch 138/200\n",
      "1562/1562 [==============================] - 193s - loss: 0.8985 - acc: 0.7137 - val_loss: 0.6965 - val_acc: 0.7781\n",
      "Epoch 139/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.8935 - acc: 0.7141 - val_loss: 0.7942 - val_acc: 0.7694\n",
      "Epoch 140/200\n",
      "1562/1562 [==============================] - 197s - loss: 0.9038 - acc: 0.7107 - val_loss: 0.8556 - val_acc: 0.7623\n",
      "Epoch 141/200\n",
      "1562/1562 [==============================] - 200s - loss: 0.9090 - acc: 0.7092 - val_loss: 0.7845 - val_acc: 0.7613\n",
      "Epoch 142/200\n",
      "1562/1562 [==============================] - 202s - loss: 0.9107 - acc: 0.7074 - val_loss: 0.7675 - val_acc: 0.7554\n",
      "Epoch 143/200\n",
      "1562/1562 [==============================] - 203s - loss: 0.9241 - acc: 0.7066 - val_loss: 0.8905 - val_acc: 0.7502\n",
      "Epoch 144/200\n",
      "1562/1562 [==============================] - 205s - loss: 0.9347 - acc: 0.7013 - val_loss: 0.8041 - val_acc: 0.7370\n",
      "Epoch 145/200\n",
      "1562/1562 [==============================] - 206s - loss: 0.9407 - acc: 0.7019 - val_loss: 0.8707 - val_acc: 0.7152\n",
      "Epoch 146/200\n",
      "1562/1562 [==============================] - 197s - loss: 0.9476 - acc: 0.6987 - val_loss: 0.8029 - val_acc: 0.7554\n",
      "Epoch 147/200\n",
      "1562/1562 [==============================] - 192s - loss: 0.9473 - acc: 0.6980 - val_loss: 0.7589 - val_acc: 0.7518\n",
      "Epoch 148/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.9703 - acc: 0.6917 - val_loss: 0.9669 - val_acc: 0.7021\n",
      "Epoch 149/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.9784 - acc: 0.6882 - val_loss: 0.9280 - val_acc: 0.6874\n",
      "Epoch 150/200\n",
      "1562/1562 [==============================] - 191s - loss: 0.9921 - acc: 0.6841 - val_loss: 0.8058 - val_acc: 0.7444\n",
      "Epoch 151/200\n",
      "1562/1562 [==============================] - 198s - loss: 0.9898 - acc: 0.6841 - val_loss: 0.9384 - val_acc: 0.6964\n",
      "Epoch 152/200\n",
      "1562/1562 [==============================] - 199s - loss: 1.0033 - acc: 0.6825 - val_loss: 0.8974 - val_acc: 0.7173\n",
      "Epoch 153/200\n",
      "1562/1562 [==============================] - 200s - loss: 1.0155 - acc: 0.6771 - val_loss: 1.2795 - val_acc: 0.6417\n",
      "Epoch 154/200\n",
      "1562/1562 [==============================] - 204s - loss: 1.0310 - acc: 0.6727 - val_loss: 1.0887 - val_acc: 0.6674\n",
      "Epoch 155/200\n",
      "1562/1562 [==============================] - 200s - loss: 1.0382 - acc: 0.6714 - val_loss: 1.0582 - val_acc: 0.7140\n",
      "Epoch 156/200\n",
      "1562/1562 [==============================] - 191s - loss: 1.0412 - acc: 0.6691 - val_loss: 0.9888 - val_acc: 0.7069\n",
      "Epoch 157/200\n",
      "1562/1562 [==============================] - 196s - loss: 1.0426 - acc: 0.6675 - val_loss: 1.2037 - val_acc: 0.6383\n",
      "Epoch 158/200\n",
      "1562/1562 [==============================] - 196s - loss: 1.0556 - acc: 0.6692 - val_loss: 1.0427 - val_acc: 0.6764\n",
      "Epoch 159/200\n",
      "1562/1562 [==============================] - 197s - loss: 1.0648 - acc: 0.6619 - val_loss: 1.0070 - val_acc: 0.7139\n",
      "Epoch 160/200\n",
      "1562/1562 [==============================] - 200s - loss: 1.0671 - acc: 0.6630 - val_loss: 0.9262 - val_acc: 0.6939\n",
      "Epoch 161/200\n",
      "1562/1562 [==============================] - 200s - loss: 1.0745 - acc: 0.6583 - val_loss: 1.0271 - val_acc: 0.6978\n",
      "Epoch 162/200\n",
      "1562/1562 [==============================] - 197s - loss: 1.0896 - acc: 0.6541 - val_loss: 0.9510 - val_acc: 0.7126\n",
      "Epoch 163/200\n",
      "1562/1562 [==============================] - 200s - loss: 1.0906 - acc: 0.6544 - val_loss: 0.9478 - val_acc: 0.6932\n",
      "Epoch 164/200\n",
      "1562/1562 [==============================] - 194s - loss: 1.0960 - acc: 0.6515 - val_loss: 0.8709 - val_acc: 0.7209\n",
      "Epoch 165/200\n",
      "1562/1562 [==============================] - 199s - loss: 1.1042 - acc: 0.6500 - val_loss: 1.0705 - val_acc: 0.6529\n",
      "Epoch 166/200\n",
      "1562/1562 [==============================] - 200s - loss: 1.1154 - acc: 0.6517 - val_loss: 1.1602 - val_acc: 0.6805\n",
      "Epoch 167/200\n",
      "1562/1562 [==============================] - 199s - loss: 1.1073 - acc: 0.6484 - val_loss: 1.1176 - val_acc: 0.6983\n",
      "Epoch 168/200\n",
      "1562/1562 [==============================] - 196s - loss: 1.1200 - acc: 0.6473 - val_loss: 1.1777 - val_acc: 0.6386\n",
      "Epoch 169/200\n",
      "1562/1562 [==============================] - 199s - loss: 1.1372 - acc: 0.6416 - val_loss: 0.9736 - val_acc: 0.6822\n",
      "Epoch 170/200\n",
      "1562/1562 [==============================] - 202s - loss: 1.1290 - acc: 0.6411 - val_loss: 1.0762 - val_acc: 0.6823\n",
      "Epoch 171/200\n",
      "1562/1562 [==============================] - 197s - loss: 1.1400 - acc: 0.6379 - val_loss: 1.1266 - val_acc: 0.6518\n",
      "Epoch 172/200\n",
      "1562/1562 [==============================] - 200s - loss: 1.1563 - acc: 0.6366 - val_loss: 1.1037 - val_acc: 0.6339\n",
      "Epoch 173/200\n",
      "1562/1562 [==============================] - 199s - loss: 1.1630 - acc: 0.6310 - val_loss: 1.0363 - val_acc: 0.6487\n",
      "Epoch 174/200\n",
      "1562/1562 [==============================] - 199s - loss: 1.1545 - acc: 0.6335 - val_loss: 0.9891 - val_acc: 0.6992\n",
      "Epoch 175/200\n",
      "1562/1562 [==============================] - 201s - loss: 1.1724 - acc: 0.6294 - val_loss: 1.0177 - val_acc: 0.7027\n",
      "Epoch 176/200\n",
      "1562/1562 [==============================] - 198s - loss: 1.1794 - acc: 0.6245 - val_loss: 1.0965 - val_acc: 0.6480\n",
      "Epoch 177/200\n",
      "1562/1562 [==============================] - 197s - loss: 1.1765 - acc: 0.6308 - val_loss: 1.1336 - val_acc: 0.6402\n",
      "Epoch 178/200\n",
      "1562/1562 [==============================] - 195s - loss: 1.1964 - acc: 0.6233 - val_loss: 1.5230 - val_acc: 0.4589\n",
      "Epoch 179/200\n",
      "1562/1562 [==============================] - 195s - loss: 1.2055 - acc: 0.6167 - val_loss: 1.1148 - val_acc: 0.6269\n",
      "Epoch 180/200\n",
      "1562/1562 [==============================] - 200s - loss: 1.2083 - acc: 0.6181 - val_loss: 1.0529 - val_acc: 0.6579\n",
      "Epoch 181/200\n",
      "1562/1562 [==============================] - 198s - loss: 1.2056 - acc: 0.6186 - val_loss: 1.2197 - val_acc: 0.5697\n",
      "Epoch 182/200\n",
      "1562/1562 [==============================] - 200s - loss: 1.2106 - acc: 0.6159 - val_loss: 1.0384 - val_acc: 0.6735\n",
      "Epoch 183/200\n",
      "1562/1562 [==============================] - 199s - loss: 1.2255 - acc: 0.6141 - val_loss: 1.1197 - val_acc: 0.6593\n",
      "Epoch 184/200\n",
      "1562/1562 [==============================] - 203s - loss: 1.2211 - acc: 0.6153 - val_loss: 0.9918 - val_acc: 0.6889\n",
      "Epoch 185/200\n",
      "1562/1562 [==============================] - 203s - loss: 1.2195 - acc: 0.6176 - val_loss: 1.1135 - val_acc: 0.6563\n",
      "Epoch 186/200\n",
      "1562/1562 [==============================] - 206s - loss: 1.2249 - acc: 0.6136 - val_loss: 0.9324 - val_acc: 0.6950\n",
      "Epoch 187/200\n",
      "1562/1562 [==============================] - 204s - loss: 1.2180 - acc: 0.6134 - val_loss: 1.0223 - val_acc: 0.6815\n",
      "Epoch 188/200\n",
      "1562/1562 [==============================] - 204s - loss: 1.2241 - acc: 0.6115 - val_loss: 1.0500 - val_acc: 0.6704\n",
      "Epoch 189/200\n",
      "1562/1562 [==============================] - 207s - loss: 1.2391 - acc: 0.6074 - val_loss: 0.9866 - val_acc: 0.6806\n",
      "Epoch 190/200\n",
      "1562/1562 [==============================] - 203s - loss: 1.2479 - acc: 0.6055 - val_loss: 1.1492 - val_acc: 0.6340\n",
      "Epoch 191/200\n",
      "1562/1562 [==============================] - 205s - loss: 1.2434 - acc: 0.6044 - val_loss: 1.0838 - val_acc: 0.6735\n",
      "Epoch 192/200\n",
      "1562/1562 [==============================] - 206s - loss: 1.2441 - acc: 0.6042 - val_loss: 1.1963 - val_acc: 0.6085\n",
      "Epoch 193/200\n",
      "1562/1562 [==============================] - 203s - loss: 1.2523 - acc: 0.6024 - val_loss: 1.2929 - val_acc: 0.5567\n",
      "Epoch 194/200\n",
      "1562/1562 [==============================] - 204s - loss: 1.2490 - acc: 0.6043 - val_loss: 0.9948 - val_acc: 0.7036\n",
      "Epoch 195/200\n",
      "1562/1562 [==============================] - 204s - loss: 1.2499 - acc: 0.6011 - val_loss: 1.1344 - val_acc: 0.6559\n",
      "Epoch 196/200\n",
      "1562/1562 [==============================] - 202s - loss: 1.2673 - acc: 0.5990 - val_loss: 1.1719 - val_acc: 0.6001\n",
      "Epoch 197/200\n",
      "1562/1562 [==============================] - 202s - loss: 1.2778 - acc: 0.5956 - val_loss: 1.4352 - val_acc: 0.5163\n",
      "Epoch 198/200\n",
      "1562/1562 [==============================] - 199s - loss: 1.2897 - acc: 0.5914 - val_loss: 1.2326 - val_acc: 0.6167\n",
      "Epoch 199/200\n",
      "1562/1562 [==============================] - 202s - loss: 1.2933 - acc: 0.5913 - val_loss: 1.4839 - val_acc: 0.5578\n",
      "Epoch 200/200\n",
      "1562/1562 [==============================] - 205s - loss: 1.2962 - acc: 0.5903 - val_loss: 1.1413 - val_acc: 0.6293\n"
     ]
    }
   ],
   "source": [
    "# If we're using data augmentation, we are going to fit our model with our training data, and our batch size, as well as our epochs.\n",
    "# We're also using some validation_data to make sure our model works fine\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "     print('Using real-time data augmentation.')\n",
    "     datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=0,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False\n",
    "     )\n",
    "     datagen.fit(x_train)\n",
    "\n",
    "     # Fit the model on the batches generated by datagen.flow().\n",
    "     model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Executing the above will train the model and will take some time. A few hours with a decent laptop via Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /Users/ryan/GitHub/AITesting/Misc./saved_models/keras_first_cnn_test.h5 \n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 1.14125301056\n",
      "Test accuracy: 0.6293\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "model_path = os.path.join(save_directory, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "Our outputed accuracy is about 0.63. Which is terrible, but with better models, we can improve that accuracy.\n",
    "\n",
    "Original Source code located:\n",
    "https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
